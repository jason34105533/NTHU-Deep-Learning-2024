{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID, name.\n",
    "\n",
    "111062201 林威盛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data\n",
    "\n",
    "Personally, I extract time stamp, length of content, number of images, num of external url link, channel. Meanwhile apply bag of words in topic, author,and title, and use them as feature.\n",
    "\n",
    "For time stamp, use re.find<tag> to get the string and use string split to get the year, month, day, hour, minute, second. Then use datetime to convert the string to weekday.\n",
    "As for channel, use re.find<tag> to get the string and call funmction pd.Categorical to convert the string to number as feature.\n",
    "\n",
    "For the length of content, number of images, num of external url link, use Bs4 to locate the tag and count them.\n",
    "\n",
    "As for the bag of words, use Bs4 to get the string in <tag>. Then finally use CountVectorizer applying tokenizer_stem_nostop to convert the words to feature.\n",
    "\n",
    "One crucial point is that the number of author is not always just one, so use split and append all the names in <author_name>, <article-info> into a list.\n",
    "I firstly handle author name by CategoryEncoder, but the result is not good. \n",
    "And I find that the author name is not always the same, so I think use BOW will be better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sysadmin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/sysadmin/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tag done\n",
      "time stamp done\n",
      "channel done\n",
      "Concat done\n",
      "df_tag done\n",
      "time stamp done\n",
      "channel done\n",
      "Concat done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import ast\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "# print(df.head(2))\n",
    "# # print(df.loc[0,'Page content'])\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tag_extractor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # find author\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author_name = article_info.find('span', {'class': 'author_name'})\n",
    "    if author_name != None:\n",
    "        author = author_name.get_text()\n",
    "    elif article_info.span != None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "        \n",
    "    # clean author\n",
    "    author = re.sub('\\s+', ' ', author.strip().lower())\n",
    "    if author.startswith('by '):\n",
    "        author = author[3:]\n",
    "    author = re.sub('&.*;', '&', author.replace(' and ', ' & '))\n",
    "\n",
    "    author_list = []\n",
    "    if author.find(',') == -1:\n",
    "        author_list = re.split('\\s*&\\s*', author)\n",
    "    else:\n",
    "        authors = re.split('\\s*,\\s*', author)\n",
    "        if authors[-1].find('&') == -1 or len(authors[-1].split('&')[-1].strip().split()) > 3:\n",
    "            author_list.append(authors[0])\n",
    "        else:\n",
    "            author_list += authors[:-1]\n",
    "            author_list += re.split('\\s*&\\s*', authors[-1])\n",
    "    author = ' '.join([re.sub('\\s+', '_', a) for a in author_list])\n",
    "\n",
    "    # find content\n",
    "    content = soup.body.find('section', {'class': 'article-content'}).get_text()\n",
    "    content_len = len(content)\n",
    "\n",
    "    # find image\n",
    "    num_image = len(soup.body.find_all('img'))\n",
    "\n",
    "    # find a\n",
    "    num_a = len(soup.body.find_all('a'))\n",
    "    \n",
    "    # find title\n",
    "    title = soup.body.h1.string.strip().lower()\n",
    "    \n",
    "    # find topic\n",
    "    a_list = soup.body.find('footer', {'class': 'article-topics'}).find_all('a')\n",
    "    topic_list = [a.string.strip().lower() for a in a_list]\n",
    "    topic = ' '.join([re.sub('\\s+', '_', t) for t in topic_list])\n",
    "\n",
    "    return author, content_len, num_image, num_a, title, topic\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "\n",
    "'''\n",
    "`count.inverse_transform(np.ones((1, bag_cnts.shape[0])))[0]` returns a list of all vocabulary in the corpus\n",
    "`bag_cnts.argsort()[::-1]` returns a list of indices of sorted bag_cnts in descending order (This List of Indices could sort `bag_cnts` in descending order)\n",
    "'''\n",
    "def get_sentiment(text):\n",
    "    \n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    scores = analyzer.polarity_scores(text)\n",
    "\n",
    "    return scores['pos']\n",
    "    \n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def feature_extractor(df):\n",
    "    '''\n",
    "    @param df: DataFrame\n",
    "    '''\n",
    "    # df : Id, Popularity, Page content (Popularity is the target variable to predict)\n",
    "    # I want to get the number of url, the number of images and the publish year,month,day, hour, and weekday,and which channel the article is in\n",
    "    # Implement your code here\n",
    "    \n",
    "    df_tag = []\n",
    "    for i in range(len(df)):\n",
    "        df_tag.append(tag_extractor(df.loc[i,'Page content']))\n",
    "    \n",
    "    df_tag = pd.DataFrame(df_tag, columns=['author','content_len', 'num_image', 'num_url', 'title', 'topic'])\n",
    "    # df_tag['author'] = pd.Categorical(df_tag['author']).codes\n",
    "    \n",
    "    # df_tag.to_csv('author.csv', columns=['author'], index=False)\n",
    "\n",
    "    print(\"df_tag done\")\n",
    "    \n",
    "    # The input has <time> tag <time datetime=\"2014-09-19T02:07:46+00:00\">2014-09-19 02:07:46 UTC</time>\n",
    "    tmp_time = df['Page content'].apply(lambda x: re.findall(r'<time datetime=\".*?\">(.*?)</time>', x))\n",
    "    \n",
    "    # Some items in tmp_time are something like this:\"['2014-12-11 12:52:05 UTC', 'Dec 12, 2014 at 1:36am PST', 'Dec 12, 2014 at 1:37am PST', 'Dec 12, 2014 at 1:56am PST', 'Dec 12, 2014 at 2:05am PST', 'Dec 12, 2014 at 2:11am PST']\"\n",
    "    tmp_time = tmp_time.apply(lambda x: x if type(x) == list else ast.literal_eval(x)[0])\n",
    "    \n",
    "    # tmp_time is like :\"2013-06-19 15:04:30 UTC\" , handle the case when tmp_time is empty\n",
    "    for i in range(len(tmp_time)):\n",
    "        if type(tmp_time[i]) == list and len(tmp_time[i]) ==  0:\n",
    "            tmp_time[i] = ['2013-06-19 15:04:30 UTC']\n",
    "            \n",
    "    \n",
    "    df['year'] = tmp_time.apply(lambda x: int(x[0].split('-')[0]))\n",
    "    df['month'] = tmp_time.apply(lambda x: int(x[0].split('-')[1]))\n",
    "    df['day'] = tmp_time.apply(lambda x: int((x[0].split('-')[2]).split(' ')[0]))\n",
    "    df['hour'] = tmp_time.apply(lambda x: int((x[0].split(' ')[1]).split(':')[0]))\n",
    "    df['minute'] = tmp_time.apply(lambda x: int((x[0].split(' ')[1]).split(':')[1]))\n",
    "    df['second'] = tmp_time.apply(lambda x: int((x[0].split(' ')[1]).split(':')[2]))\n",
    "    \n",
    "    df['weekday'] = tmp_time.apply(lambda x: int(pd.to_datetime(x[0]).weekday()))\n",
    "    \n",
    "    print(\"time stamp done\")\n",
    "    \n",
    "    # There is a tag in Page content <article data-channel=\"\"tech\"\">\n",
    "    df['channel'] = df['Page content'].apply(lambda x: re.findall(r'<article data-channel=\"(.*?)\"', x))\n",
    "    df['channel'] = df['channel'].apply(lambda x: x[0] if len(x) > 0 else 'unknown')\n",
    "    # change channel into categorical data\n",
    "    df['channel'] = pd.Categorical(df['channel']).codes\n",
    "    print(\"channel done\")\n",
    "    \n",
    "    # concat df and df_tag\n",
    "    df = pd.concat([df, df_tag], axis=1)\n",
    "    print(\"Concat done\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "new_df = feature_extractor(df)\n",
    "\n",
    "new_df.to_csv('new_df.csv', index=False)\n",
    "\n",
    "# Remove the Page content column\n",
    "new_df = new_df.drop(columns=['Page content'])\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('./test.csv')\n",
    "new_df_test = feature_extractor(df_test)\n",
    "new_df_test = new_df_test.drop(columns=['Page content']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the classifier\n",
    "\n",
    "For these such amount of features after applying BOW, I consider use Random Forest to build the classifier would be a good choice.\n",
    "I select LGBM and XGBoost as the classifier and a VotingClassifier to combine them, with weights [1, 0.5].\n",
    "I have tried further combine them with simple LogisticRegression, but the result hasn't been improved, So it just ends up with two-model-voting system.\n",
    "\n",
    "For tuning the model I lower the learning rate to 0.008 to make the model more robust.\n",
    "And, increase the number of estimators to 500, which is better enough to have the optimal result without overfitting,.\n",
    "Apply regularization to achieve better generalizability.\n",
    "\n",
    "For daily testing, set the training set and validation set as the first 80% and the last 20% of the data, respectively.\n",
    "And for the \"LAST SHOT\", I use almost the whole data to train the model and predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id  Popularity  year  month  day  hour  minute  second  weekday  \\\n",
      "0  12822          -1  2013      5    3    14      25      22        4   \n",
      "1  11291           1  2013      2   20    15       9      48        2   \n",
      "2  21366           1  2013      4   16    22      11      57        1   \n",
      "3  23680           1  2014      5   22    19      47       1        3   \n",
      "4    121          -1  2014      2   25    23      19       8        1   \n",
      "\n",
      "   channel          author  content_len  num_image  num_url  \\\n",
      "0       10   chelsea_stark         2206          1       13   \n",
      "1       28  stan_schroeder         1696          2        8   \n",
      "2       29   annie_colbert         1836          1       41   \n",
      "3       31   brian_koerber         1572          0       31   \n",
      "4       28    chris_taylor         1942          1       14   \n",
      "\n",
      "                                               title  \\\n",
      "0  questing meets candy in this odd ascii browser...   \n",
      "1  yahoo redesigns homepage, adds infinite scrolling   \n",
      "2  10 powerful images of nyc showing its love for...   \n",
      "3  even asylum in russia can't save snowden from ...   \n",
      "4  tesla model s is top car of 2014, 'consumer re...   \n",
      "\n",
      "                                               topic  \n",
      "0             ascii entertainment gaming weird_games  \n",
      "1                   dev_&_design redesign tech yahoo  \n",
      "2  boston boston_marathon new_york_city photograp...  \n",
      "3  brian_williams edward_snowden memes nbc photos...  \n",
      "4               cars tech tesla_motors tesla_model_s  \n",
      "new_df done !\n",
      "Start to train the Model \n",
      "lgbm AUROC score\n",
      "0.599 (+/-0.008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost AUROC score\n",
      "0.594 (+/-0.008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/sysadmin/miniconda3/envs/myenv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC score\n",
      "0.599 (+/-0.008)\n",
      ":Training done ! Validation start !\n",
      "Predict the Popularity of test data\n",
      "Write the prediction to a csv file\n"
     ]
    }
   ],
   "source": [
    "trans = ColumnTransformer(\n",
    "    [('Author', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False,dtype=float,token_pattern=None),'author')\n",
    "     ,('Topic', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False,dtype=float,token_pattern=None),'topic')\n",
    "     ,('Title', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False,dtype=float,token_pattern=None),'title'),\n",
    "     ],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(new_df.head(5))\n",
    "print(\"new_df done !\")\n",
    "\n",
    "# This time using whole dataset, with preprocessing\n",
    "# Id,Popularity,Page content\n",
    "# split df into training and validation with ratio of 80:20\n",
    "\n",
    "# shuffle the data\n",
    "new_df = new_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_train = new_df.iloc[:int(len(new_df)*0.999)]\n",
    "df_valid = new_df.iloc[int(len(new_df)*0.999):]\n",
    "\n",
    "x_train = df_train.drop(columns=['Popularity','Id'])\n",
    "y_train = df_train['Popularity']\n",
    "y_train = y_train.apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# df_train.to_csv('df_train.csv', index=False)\n",
    "\n",
    "print(\"Start to train the Model \")\n",
    "\n",
    "# lgbm\n",
    "lgbm = Pipeline([('ct', trans),\n",
    "                 ('clf', LGBMClassifier(n_estimators=500, verbosity=-1, learning_rate=0.008, reg_alpha=0.1, reg_lambda=0.1))])\n",
    "lgbm.fit(x_train, y_train)\n",
    "\n",
    "score = cross_val_score(lgbm, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(\"lgbm AUROC score\")\n",
    "print('%.3f (+/-%.3f)' % (score.mean(), score.std()))\n",
    "\n",
    "\n",
    "# other models\n",
    "\n",
    "# xgboost\n",
    "xgboost = Pipeline([('ct', trans),\n",
    "                    ('clf', XGBClassifier(verbosity=0, n_estimators=500, learning_rate = 0.01, reg_alpha=0.1, reg_lambda=0.1))])\n",
    "xgboost.fit(x_train, y_train)\n",
    "\n",
    "score = cross_val_score(xgboost, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(\"xgboost AUROC score\")\n",
    "print('%.3f (+/-%.3f)' % (score.mean(), score.std()))\n",
    "\n",
    "# # SVM\n",
    "# linear_svc = OneVsRestClassifier(LinearSVC())\n",
    "# calibrated_clf = CalibratedClassifierCV(\n",
    "#            estimator=linear_svc,\n",
    "#            cv=\"prefit\")\n",
    "# svm = Pipeline([('ct',trans),\n",
    "#                 ('clf', linear_svc)])\n",
    "# svm.fit(x_train, y_train)\n",
    "# # OneVsRestClassifier(LinearSVC())\n",
    "# score = cross_val_score(svm, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "# print(\"svm AUROC score\")\n",
    "# print('%.3f (+/-%.3f)' % (score.mean(), score.std()))\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "logit = Pipeline([('ct', trans),\n",
    "                ('clf', LogisticRegression(solver='liblinear',max_iter=1000, C=0.1))])\n",
    "# logit.fit(x_train, y_train)\n",
    "\n",
    "# score = cross_val_score(logit, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "# print(\"logit AUROC score\")\n",
    "# print('%.3f (+/-%.3f)' % (score.mean(), score.std()))\n",
    "\n",
    "# voting\n",
    "voting = VotingClassifier([('lgbm', lgbm),('xgboost', xgboost),('logit',logit)],\n",
    "                          voting='soft', weights=[1, 0.5,0])\n",
    "voting.fit(x_train, y_train)\n",
    "\n",
    "score = cross_val_score(voting, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(\"AUROC score\")\n",
    "print('%.3f (+/-%.3f)' % (score.mean(), score.std()))\n",
    "\n",
    "# exit(0)\n",
    "\n",
    "\n",
    "print(\":Training done ! Validation start !\")\n",
    "\n",
    "# Predict the Popularity of test data\n",
    "print(\"Predict the Popularity of test data\")\n",
    "\n",
    "pred_proba = lgbm.predict_proba(new_df_test.drop(columns=['Id']))[:,1]\n",
    "\n",
    "# Save the prediction to a csv file\n",
    "print(\"Write the prediction to a csv file\")\n",
    "df_test['Popularity'] = pred_proba\n",
    "df_test.to_csv('submission.csv', columns=['Id', 'Popularity'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In this competition, I have tried to extract the features from the data and build the classifier with Random Forest, LGBM, and XGBoost.\n",
    "The result is not good enough, and just like what professor said, the most time-consuming part is to preprocess data and feature engineering lol.\n",
    "The LGBM model do provide \"Feature importance\" to demonstrate the actual value of a designed feature, and I think it is a good way to further improve the model by eliminating some of the redundant feature to achieve better prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
